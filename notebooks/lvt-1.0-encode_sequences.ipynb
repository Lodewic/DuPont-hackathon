{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we need tensorflow and keras at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.enzyme_hackathon.utils import (one_hot_dict, one_hot_encode, one_hot_encode_screening_data,\n",
    "                                        one_hot_encode_sequences)\n",
    "from src.enzyme_hackathon.utils import AA_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.enzyme_hackathon.utils import (create_fc_model, predict, generate_variant, r_squared, nan_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  performance    stability  productivity      fitness\n",
      "count  7000.000000  6896.000000  6878.000000   7000.000000  6878.000000\n",
      "mean   4010.524714     0.984543     0.781275      1.215665     0.898539\n",
      "std    2299.783102     0.283531     0.324650      0.462164     0.491000\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%    2026.750000     0.911732     0.499931      0.992109     0.494338\n",
      "50%    4000.500000     0.997816     0.841235      1.016482     0.891317\n",
      "75%    5999.250000     1.169030     1.000279      1.501715     1.266698\n",
      "max    7999.000000     2.253050     2.493611      3.035927     1.940561         Unnamed: 0  performance    stability  productivity      fitness\n",
      "count  9906.000000  9838.000000  9719.000000   9906.000000  9906.000000\n",
      "mean   5001.255704     1.077744     1.093033      1.173327     1.330397\n",
      "std    2884.806712     0.331799     0.317386      0.356221     0.515413\n",
      "min       1.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%    2503.250000     0.979577     0.981840      0.984759     0.995742\n",
      "50%    5001.500000     1.004226     1.008424      1.014753     1.392143\n",
      "75%    7498.750000     1.031376     1.045084      1.466514     1.650875\n",
      "max    9999.000000     2.091435     2.096095      2.138840     2.496123         Unnamed: 0  productivity  performance    stability      fitness\n",
      "count  8000.000000   8000.000000  6730.000000  5336.000000  5336.000000\n",
      "mean   4505.748125      1.733945     0.846407     1.425351     1.240450\n",
      "std    2599.446978      2.100859     1.627194     1.783363     1.894834\n",
      "min       0.000000      0.000000     0.000000     0.000000     0.000000\n",
      "25%    2256.750000      0.043985     0.014479     0.712580     0.007385\n",
      "50%    4505.500000      0.890866     0.098402     0.981191     0.161588\n",
      "75%    6763.250000      4.023183     0.945701     1.068888     1.227807\n",
      "max    8999.000000      6.948362     7.280907     7.596844     6.975504\n"
     ]
    }
   ],
   "source": [
    "regression_df = pd.read_csv(\"../data/raw/challenge-0.csv\")\n",
    "df1 = pd.read_csv(\"../data/raw/challenge-1.csv\")\n",
    "df2 = pd.read_csv(\"../data/raw/challenge-2.csv\")\n",
    "print(regression_df.describe(), df1.describe(), df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "encoded_split_dict = one_hot_encode_screening_data(regression_df)\n",
    "x_train, x_test = (encoded_split_dict['x'][0], encoded_split_dict['x'][1])\n",
    "y_keys = [key for key in encoded_split_dict['y'].keys()]\n",
    "\n",
    "y_train, y_test = (OrderedDict(), OrderedDict())\n",
    "for y_key in y_keys:\n",
    "    y_train[y_key], y_test[y_key] = (encoded_split_dict['y'][y_key]['data'][0], encoded_split_dict['y'][y_key]['data'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for the challenge 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def create_fc_model(data: dict):\n",
    "    \"\"\"Create a multi-task fully connected neural network model\n",
    "\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sequence_input = Input(shape=data['x'][0][0].shape, name='sequence')\n",
    "    sequence = Flatten()(sequence_input)\n",
    "\n",
    "    sequence = Dense(50, activation='relu', kernel_regularizer=l2(1.1))(sequence)\n",
    "    sequence = Dropout(.1)(sequence)\n",
    "\n",
    "    outputs = []\n",
    "    for task, _ in data['y'].items():\n",
    "        t = Dense(20, activation='relu', kernel_regularizer=l2(1.1))(sequence)\n",
    "        t = Dropout(.1)(t)\n",
    "        t = BatchNormalization()(t)\n",
    "        outputs.append(Dense(1, name=task)(t))\n",
    "\n",
    "    model = Model(inputs=[sequence_input], outputs=outputs)\n",
    "    model.compile(loss=nan_mse, optimizer=Adam(lr=.0001), metrics=[r_squared])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_nn = create_fc_model(encoded_split_dict)\n",
    "# for key in y_test.keys():\n",
    "#     y_train_tuples = [(x.tolist()[0], y.tolist()[0], z.tolist()[0]) for x, y, z in zip(y_train[y_keys[0]], y_train[y_keys[1]], y_train[y_keys[2]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5250/5250 [==============================] - 1s 135us/step - loss: 47.4171 - productivity_loss: 0.3475 - performance_loss: 0.2375 - stability_loss: 0.2815 - productivity_r_squared: 0.6536 - performance_r_squared: 0.7527 - stability_r_squared: 0.7160\n",
      "Epoch 2/50\n",
      "5250/5250 [==============================] - 1s 144us/step - loss: 44.7153 - productivity_loss: 0.3351 - performance_loss: 0.2310 - stability_loss: 0.2580 - productivity_r_squared: 0.6650 - performance_r_squared: 0.7608 - stability_r_squared: 0.7384\n",
      "Epoch 3/50\n",
      "5250/5250 [==============================] - 1s 136us/step - loss: 42.1702 - productivity_loss: 0.3298 - performance_loss: 0.2191 - stability_loss: 0.2543 - productivity_r_squared: 0.6701 - performance_r_squared: 0.7739 - stability_r_squared: 0.7415\n",
      "Epoch 4/50\n",
      "5250/5250 [==============================] - 1s 128us/step - loss: 39.7541 - productivity_loss: 0.3136 - performance_loss: 0.2207 - stability_loss: 0.2486 - productivity_r_squared: 0.6850 - performance_r_squared: 0.7714 - stability_r_squared: 0.7469\n",
      "Epoch 5/50\n",
      "5250/5250 [==============================] - 1s 125us/step - loss: 37.4298 - productivity_loss: 0.3038 - performance_loss: 0.1975 - stability_loss: 0.2344 - productivity_r_squared: 0.6977 - performance_r_squared: 0.7940 - stability_r_squared: 0.7631\n",
      "Epoch 6/50\n",
      "5250/5250 [==============================] - 1s 124us/step - loss: 35.2325 - productivity_loss: 0.2901 - performance_loss: 0.1891 - stability_loss: 0.2215 - productivity_r_squared: 0.7098 - performance_r_squared: 0.8049 - stability_r_squared: 0.7725\n",
      "Epoch 7/50\n",
      "5250/5250 [==============================] - 1s 128us/step - loss: 33.1732 - productivity_loss: 0.2883 - performance_loss: 0.1843 - stability_loss: 0.2212 - productivity_r_squared: 0.7141 - performance_r_squared: 0.8059 - stability_r_squared: 0.7743\n",
      "Epoch 8/50\n",
      "5250/5250 [==============================] - 1s 127us/step - loss: 31.2153 - productivity_loss: 0.2899 - performance_loss: 0.1899 - stability_loss: 0.2032 - productivity_r_squared: 0.7119 - performance_r_squared: 0.8022 - stability_r_squared: 0.7946\n",
      "Epoch 9/50\n",
      "5250/5250 [==============================] - 1s 123us/step - loss: 29.3396 - productivity_loss: 0.2711 - performance_loss: 0.1856 - stability_loss: 0.1979 - productivity_r_squared: 0.7298 - performance_r_squared: 0.8108 - stability_r_squared: 0.7995\n",
      "Epoch 10/50\n",
      "5250/5250 [==============================] - 1s 128us/step - loss: 27.5866 - productivity_loss: 0.2709 - performance_loss: 0.1811 - stability_loss: 0.2012 - productivity_r_squared: 0.7272 - performance_r_squared: 0.8125 - stability_r_squared: 0.7945\n",
      "Epoch 11/50\n",
      "5250/5250 [==============================] - 1s 135us/step - loss: 25.9064 - productivity_loss: 0.2656 - performance_loss: 0.1721 - stability_loss: 0.1951 - productivity_r_squared: 0.7366 - performance_r_squared: 0.8211 - stability_r_squared: 0.8002\n",
      "Epoch 12/50\n",
      "5250/5250 [==============================] - 1s 127us/step - loss: 24.3242 - productivity_loss: 0.2614 - performance_loss: 0.1717 - stability_loss: 0.1887 - productivity_r_squared: 0.7384 - performance_r_squared: 0.8223 - stability_r_squared: 0.8071\n",
      "Epoch 13/50\n",
      "5250/5250 [==============================] - 1s 129us/step - loss: 22.8353 - productivity_loss: 0.2677 - performance_loss: 0.1661 - stability_loss: 0.1870 - productivity_r_squared: 0.7337 - performance_r_squared: 0.8277 - stability_r_squared: 0.8094\n",
      "Epoch 14/50\n",
      "5250/5250 [==============================] - 1s 129us/step - loss: 21.4161 - productivity_loss: 0.2596 - performance_loss: 0.1665 - stability_loss: 0.1840 - productivity_r_squared: 0.7397 - performance_r_squared: 0.8266 - stability_r_squared: 0.8118\n",
      "Epoch 15/50\n",
      "5250/5250 [==============================] - 1s 130us/step - loss: 20.0796 - productivity_loss: 0.2580 - performance_loss: 0.1692 - stability_loss: 0.1773 - productivity_r_squared: 0.7416 - performance_r_squared: 0.8245 - stability_r_squared: 0.8199\n",
      "Epoch 16/50\n",
      "4400/5250 [========================>.....] - ETA: 0s - loss: 18.8945 - productivity_loss: 0.2553 - performance_loss: 0.1521 - stability_loss: 0.1729 - productivity_r_squared: 0.7420 - performance_r_squared: 0.8429 - stability_r_squared: 0.8257"
     ]
    }
   ],
   "source": [
    "fc_nn.fit(x=x_train, y=[y_train[y_keys[0]], y_train[y_keys[1]], y_train[y_keys[2]]],\n",
    "         epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mfc_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Trains the model for a given number of epochs (iterations on a dataset).\n",
       "\n",
       "# Arguments\n",
       "    x: Numpy array of training data (if the model has a single input),\n",
       "        or list of Numpy arrays (if the model has multiple inputs).\n",
       "        If input layers in the model are named, you can also pass a\n",
       "        dictionary mapping input names to Numpy arrays.\n",
       "        `x` can be `None` (default) if feeding from\n",
       "        framework-native tensors (e.g. TensorFlow data tensors).\n",
       "    y: Numpy array of target (label) data\n",
       "        (if the model has a single output),\n",
       "        or list of Numpy arrays (if the model has multiple outputs).\n",
       "        If output layers in the model are named, you can also pass a\n",
       "        dictionary mapping output names to Numpy arrays.\n",
       "        `y` can be `None` (default) if feeding from\n",
       "        framework-native tensors (e.g. TensorFlow data tensors).\n",
       "    batch_size: Integer or `None`.\n",
       "        Number of samples per gradient update.\n",
       "        If unspecified, `batch_size` will default to 32.\n",
       "    epochs: Integer. Number of epochs to train the model.\n",
       "        An epoch is an iteration over the entire `x` and `y`\n",
       "        data provided.\n",
       "        Note that in conjunction with `initial_epoch`,\n",
       "        `epochs` is to be understood as \"final epoch\".\n",
       "        The model is not trained for a number of iterations\n",
       "        given by `epochs`, but merely until the epoch\n",
       "        of index `epochs` is reached.\n",
       "    verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
       "        0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
       "    callbacks: List of `keras.callbacks.Callback` instances.\n",
       "        List of callbacks to apply during training.\n",
       "        See [callbacks](/callbacks).\n",
       "    validation_split: Float between 0 and 1.\n",
       "        Fraction of the training data to be used as validation data.\n",
       "        The model will set apart this fraction of the training data,\n",
       "        will not train on it, and will evaluate\n",
       "        the loss and any model metrics\n",
       "        on this data at the end of each epoch.\n",
       "        The validation data is selected from the last samples\n",
       "        in the `x` and `y` data provided, before shuffling.\n",
       "    validation_data: tuple `(x_val, y_val)` or tuple\n",
       "        `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
       "        the loss and any model metrics at the end of each epoch.\n",
       "        The model will not be trained on this data.\n",
       "        `validation_data` will override `validation_split`.\n",
       "    shuffle: Boolean (whether to shuffle the training data\n",
       "        before each epoch) or str (for 'batch').\n",
       "        'batch' is a special option for dealing with the\n",
       "        limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
       "        Has no effect when `steps_per_epoch` is not `None`.\n",
       "    class_weight: Optional dictionary mapping class indices (integers)\n",
       "        to a weight (float) value, used for weighting the loss function\n",
       "        (during training only).\n",
       "        This can be useful to tell the model to\n",
       "        \"pay more attention\" to samples from\n",
       "        an under-represented class.\n",
       "    sample_weight: Optional Numpy array of weights for\n",
       "        the training samples, used for weighting the loss function\n",
       "        (during training only). You can either pass a flat (1D)\n",
       "        Numpy array with the same length as the input samples\n",
       "        (1:1 mapping between weights and samples),\n",
       "        or in the case of temporal data,\n",
       "        you can pass a 2D array with shape\n",
       "        `(samples, sequence_length)`,\n",
       "        to apply a different weight to every timestep of every sample.\n",
       "        In this case you should make sure to specify\n",
       "        `sample_weight_mode=\"temporal\"` in `compile()`.\n",
       "    initial_epoch: Integer.\n",
       "        Epoch at which to start training\n",
       "        (useful for resuming a previous training run).\n",
       "    steps_per_epoch: Integer or `None`.\n",
       "        Total number of steps (batches of samples)\n",
       "        before declaring one epoch finished and starting the\n",
       "        next epoch. When training with input tensors such as\n",
       "        TensorFlow data tensors, the default `None` is equal to\n",
       "        the number of samples in your dataset divided by\n",
       "        the batch size, or 1 if that cannot be determined.\n",
       "    validation_steps: Only relevant if `steps_per_epoch`\n",
       "        is specified. Total number of steps (batches of samples)\n",
       "        to validate before stopping.\n",
       "\n",
       "# Returns\n",
       "    A `History` object. Its `History.history` attribute is\n",
       "    a record of training loss values and metrics values\n",
       "    at successive epochs, as well as validation loss values\n",
       "    and validation metrics values (if applicable).\n",
       "\n",
       "# Raises\n",
       "    RuntimeError: If the model was never compiled.\n",
       "    ValueError: In case of mismatch between the provided input data\n",
       "        and what the model expects.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\anaconda3\\envs\\dupont\\lib\\site-packages\\keras\\engine\\training.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?fc_nn.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def string_vectorizer(sequence, alphabet=string.ascii_uppercase):\n",
    "    vector = [[0 if char != letter else 1 for char in alphabet] \n",
    "                  for letter in sequence.upper()]\n",
    "    return vector\n",
    "\n",
    "regression_df['encoded_sequence'] = df0['sequence'].map(string_vectorizer)\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_df['sequence_length'] = regression_df['sequence'].map(len)\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dupont]",
   "language": "python",
   "name": "conda-env-dupont-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
